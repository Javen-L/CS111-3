NAME: Anh Mac
EMAIL: anhmvc@gmail.com
UID: 905111606

SortedList.h: a header file containing interfaces for linked list operations

SortedList.c: the source for a C source module that implements insert, delete, lookup, and length methods for a sorted doubly linked list described in the header file

lab2_list.c: the source for a C program that implements specified command line options (--threads, --iterations, --yield, --sync, --lists), drives one or more parallel threads that do operations on a shared linked list, and reports on the final list and performance

Makefile:
	.. tests: run all specified test cases to generate CSV results
	.. profile: run tests with profiling tools to generate an execution profiling report
	.. graphs: use gnuplot to generate the required graphs
	.. dist: create the deliverable tarball with contents specified here
	.. clean: delete all programs and output generated by the Makefile

README: contains descriptions of each of the included files and answers to questions in the spec

QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

Answer:
In the 1 and 2-thread list tests, most CPU time is spent in the actual list methods like insertion, lookup, deletion, and length since there is no overhead (i.e. one thread waiting for another to release a lock or context switches) to execute concurrent threads. The most expensive parts of the code would be these overheads, which would occur in trials with higher threads count. With high-thread tests, the average waiting time as threads wait for a lock increased exponentially.
In the high-thread spin-lock tests, most CPU time is spent when multiple threads are constantly checking for lock availability, as when a lock is taken, it is constantly spinning until the thread holding it finishes executing in the critical section.
In the high-thread mutex tests, most CPU time is spent on context switches as threads are constantly being blocked and rescheduled when there is no mutex available for them to enter and operate on the critical section.

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

Answer:
When the spin-lock version of the list exerciser is run with a large number of threads, most CPU time is consumed by the following line of code:

   148    148   89:       while (__sync_lock_test_and_set(&currList->spinLock,1));

The result is expected as the while loop is constantly operated to check for lock availability and request the lock. When we have large number of threads, the spinning can waste lots of CPU cycles as it is unlikely for a thread to acquire a lock when it is taken. This will also further increase synchronization cost.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

Answer:
The average lock-wait time rise dramatically with the number of contending threads as with the mutex implementation, only one thread can access the shared list at a time, which forces the abundance of other threads having to wait for this one thread. So, when the number of contending threads increases, the chances for a thread to acquire a lock decreases, and so the average time increases significantly as the queues of threads being blocked increases linearly. 
It is possible for the wait time per operation to go up faster than the completion time per operation because the completion time per operation is slower since there are also other threads doing list operations in parallel with any one thread. The wait time per operation goes up faster due to the large queue of blocked threads that we have with higher contending threads.


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

Answer:
Throughput increases linearly as the number of sublists increases. Only one thread can access the shared list at a time for the mutex synchronization method. So with N partitioned lists, at most N threads can access each critical section of each sublist at the same time. So it seems reasonable to suggest that  the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. This observation is true for small number of sublists. As the number of sublists increases, the increase in throughput slow down due to limitations in hardware and only certain number of threads can run on the cores, so we can only have a finite number of core threads executing the list operations. So, futher partitioning the list with more lists would not increase throughput with hardware limitations and instead raises overheads with more locks.



